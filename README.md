# Sign Language Production

## Project Overview

Sign Language Production (SLP) has wide-ranging uses in education, translation, etc. Despite its many uses, SLP faces a number of challenges that might impact its results. For example, the accuracy of converting sentences to pose representations, or how well we can synthesize images from the pose representations. We found that Sign language images often suffer from blurriness in hands and faces. Therefore, we dedicated our efforts to the second challenge, synthesis of images. <br>

In this task, the model takes a reference image of the signer and the desired pose as input, and then generates a target image in which the signer is performing the specified pose.


## Installation

Here are the steps to install and set up the project: <br>

1. Clone the GitHub repository of this project to your local environment: <br>
```
git clone git@github.com:CYWangKL/SLP.git
```

2. Create and activate a virtual environment (optional): <br>
```
conda create --name slp
conda activate slp
```

3. Install requirements
```
cd SLP
pip install -r requirements.txt
```

## Datasets

- `SynthSL` dataset: <br>
The dataset consists of multiple tar files: `train.tar`, `base_*.tar`, and `test.tar`

- `Phoenix` dataset: <br>
The dataset consists of multiple tar files: `train.tar`, `base_*.tar`, and `test.tar`


- `Bosphorus` dataset: <br>
The dataset contains multiple tar files for training and testing, as well as a `base_*.tar` files. `base_*.tar` needs to be decompressed and placed under our `/netscratch/$USER` directory by ourselves.

<br>
Please check the codes under `DS_loader` to ensure that the paths are properly configured.



## Training the Model

- To train the model, you can use the following commands:
```
python train.py --ds_name "<dataset_name>" --which_g "<generator_type>" --ema_rate <ema_rate_value> --input_type "<input_type>"
```
> ### Parameters

- **`--ds_name`**: Specifies the dataset for training. Available options are:
  - `SynthSL`
  - `Phoenix`
  - `Bosphorus`

- **`--which_g`**: Determines the generator architecture to use. Choose from:
  - `0` through `9`

- **`--ema_rate`**: Sets the exponential moving average rate for model weights. Use a value such as:
  - `0.9999`

- **`--input_type`**: Defines the type of input data. Available options are:
  - `heatmaps`
  - `depth`
  - `segm`
  - `normal`


## Experiment Results

After executing the training script, a new directory with the format `exp_*/` will be created in the `runs/` directory. This directory contains the experiment logs and information related to the training run.

- The experiment results will be stored in the `results/` directory. 

- The trained models will be saved in the `models/` directory.

- The training loss and other metrics are recorded in the `logs/` directory. You can use TensorBoard to visualize and analyze the training progress by running the following command in the project directory:
```
tensorboard --logdir=logs/
```

## Scripts (Before Training)
1. `preprocess`: extract frames, split train/test
  ```bash
  python scripts/preprocess.py --input_dir ./dataset/how2sign-zhewen --output_dir ./dataset/customized_dataset
  ```
2. `pkl2json`: transform pkl to json (avoid version conflict of numpy)
3. `img_stats.py`: calculate the mean & std for datasets (used by normalization later)
4. training script
  ```bash
  python train.py --ds_name "customized" --which_g "7" --ema_rate 0.999 --input_type "heatmaps" --hand_l1 True
  ```

## Kps Index Choice
* My choice, Refer this code segment:
```py
# æŒ‰ç…§â€œæ‰‹éƒ¨ > é¢éƒ¨ > ä¸ŠåŠèº«â€çš„ä¼˜å…ˆçº§è¿›è¡Œé€‰æ‹©
upper_body_indices = [0, 5, 6, 7, 8]  # é¼»å­, å·¦å³è‚©, å·¦å³è‚˜
face_expression_indices = (
    list(range(27, 37)) +  # çœ‰æ¯› (10)
    list(range(37, 49)) +  # çœ¼ç› (12)
    list(range(49, 56)) +  # é¼»å­ (7)
    list(range(56, 68)) +  # å¤–å”‡ (12)
    list(range(17, 25))    # éƒ¨åˆ†è„¸é¢Šè½®å»“ (8)
)
hand_indices = list(range(91, 133))  # å·¦å³æ‰‹ (42)

# ç»„åˆå¹¶æŽ’åºï¼Œæœ€ç»ˆå¾—åˆ°ä¸€ä¸ªæœ‰åºçš„ç´¢å¼•åˆ—è¡¨
self.selected_indices = torch.tensor(sorted(
    upper_body_indices + face_expression_indices + hand_indices
))
```

* COCO-Wholebody-133-joints Index Def:
> ä»¥ä¸‹æ˜¯ COCO-WholeBody 133 joints çš„è¯¦ç»† index å¯¹åº”éƒ¨ä½åˆ’åˆ†ï¼ˆä»Ž 0 å¼€å§‹è®¡æ•°ï¼‰ï¼š
>
>  â¸»
>
>  ðŸ§â€â™‚ï¸ Bodyï¼ˆäººä½“ä¸»å¹², 17 jointsï¼‰
>
>  0â€“16
>
>  èŒƒå›´ éƒ¨ä½
>  0 é¼»å­
>  1â€“4 çœ¼ç›å’Œè€³æœµï¼ˆå·¦çœ¼ã€å³çœ¼ã€å·¦è€³ã€å³è€³ï¼‰
>  5â€“10 è‚©è†€ã€è‚˜ã€è…•ï¼ˆå·¦è‚©ã€å³è‚©ã€å·¦è‚˜ã€å³è‚˜ã€å·¦è…•ã€å³è…•ï¼‰
>  11â€“16 é«‹ã€è†ã€è¸ï¼ˆå·¦é«‹ã€å³é«‹ã€å·¦è†ã€å³è†ã€å·¦è¸ã€å³è¸ï¼‰
>
>
>  â¸»
>
>  âœ‹ Handsï¼ˆæ‰‹éƒ¨, å·¦å³å„ 21 jointsï¼‰
>
>  å·¦æ‰‹ï¼š91â€“111  
>  å³æ‰‹ï¼š112â€“132
>
>  æ¯åªæ‰‹çš„ 21 ä¸ªå…³é”®ç‚¹ä¾æ¬¡ä¸ºï¼š
>  â€¢ 0ï¼šæ‰‹è…•
>  â€¢ 1â€“4ï¼šæ‹‡æŒ‡ï¼ˆå››ä¸ªå…³èŠ‚ï¼‰
>  â€¢ 5â€“8ï¼šé£ŸæŒ‡
>  â€¢ 9â€“12ï¼šä¸­æŒ‡
>  â€¢ 13â€“16ï¼šæ— åæŒ‡
>  â€¢ 17â€“20ï¼šå°æŒ‡
>
>  â¸»
>
>  ðŸ˜Š Faceï¼ˆé¢éƒ¨, 68 jointsï¼‰
>
>  17â€“84
>
>  åŒºåŸŸ ç´¢å¼•èŒƒå›´ è¯´æ˜Ž
>  è„¸è½®å»“ 17â€“26 ä¸‹é¢Œã€è„¸é¢Š
>  çœ‰æ¯› 27â€“36 å·¦å³çœ‰æ¯›
>  çœ¼ç› 37â€“48 å·¦å³çœ¼
>  é¼»å­ 49â€“55 é¼»æ¢ä¸Žé¼»å°–
>  å˜´å”‡ 56â€“84 å¤–å”‡ä¸Žå†…å”‡è½®å»“
>
>
>  â¸»
>
>  ðŸ¦¶ Feetï¼ˆè„šéƒ¨, 6 jointsï¼‰
>
>  85â€“90
>
>  å·¦å³è„šçš„è„šè¶¾ä¸Žè„šæŽŒå…³é”®ç‚¹ã€‚
>
>  â¸»
>
>  âœ… æŽ¨èç”¨äºŽæ‰‹è¯­è¯†åˆ«çš„ç´¢å¼•å­é›†
>
>  åŒºåŸŸ ç´¢å¼•èŒƒå›´ è¯´æ˜Ž
>  ä¸ŠåŠèº«ï¼ˆèº«ä½“+å¤´éƒ¨ï¼‰ 0â€“10 é¼»ã€çœ¼ã€è€³ã€è‚©ã€è‚˜ã€è…•
>  æ‰‹éƒ¨ï¼ˆæœ€å…³é”®ï¼‰ 91â€“132 å·¦å³æ‰‹å„ 21 ç‚¹
>  é¢éƒ¨ï¼ˆå¯é€‰ï¼‰ 17â€“26 + 49â€“55 + 56â€“63 çœ‰æ¯›ã€é¼»å­ã€å˜´éƒ¨è¡¨æƒ…
>
>  è¿™æ ·æ€»å…±çº¦ 60â€“80 ä¸ªç‚¹ï¼Œè¶³ä»¥è¦†ç›–æ‰‹è¯­è¯†åˆ«çš„ä¸»è¦ä¿¡æ¯ã€‚
>
>  â¸»